import cv2
import numpy as np
import pandas as pd
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation,
                                     MaxPooling2D, Flatten, Dense, Dropout, Concatenate)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import Sequence
from filterpy.kalman import KalmanFilter
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# =============================================================================
# 1. Data Generator with Consistent Eye & Face Crop
# =============================================================================
class GazeDataGenerator(Sequence):
    """
    Expects a CSV file with columns:
    'left_eye_path', 'right_eye_path', 'face_path', 'gaze_x', 'gaze_y'
    and yields batches of augmented images.
    """
    def __init__(self, csv_file, batch_size=32, target_eye_size=(64, 64), target_face_size=(224, 224),
                 shuffle=True, augment=False):
        self.df = pd.read_csv(csv_file)
        self.batch_size = batch_size
        self.target_eye_size = target_eye_size
        self.target_face_size = target_face_size
        self.shuffle = shuffle
        self.augment = augment
        self.on_epoch_end()
        if augment:
            self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(
                rotation_range=5,
                width_shift_range=0.1,
                height_shift_range=0.1,
                brightness_range=[0.8, 1.2],
                horizontal_flip=True
            )
    
    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))
    
    def __getitem__(self, index):
        batch_df = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size]
        left_imgs, right_imgs, face_imgs, labels = [], [], [], []
        for _, row in batch_df.iterrows():
            left_img = cv2.imread(row['left_eye_path'])
            right_img = cv2.imread(row['right_eye_path'])
            face_img = cv2.imread(row['face_path'])
            # Skip if any image is missing.
            if left_img is None or right_img is None or face_img is None:
                continue
            # Convert BGR -> RGB and resize
            left_img = cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)
            right_img = cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)
            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            left_img = cv2.resize(left_img, self.target_eye_size) / 255.0
            right_img = cv2.resize(right_img, self.target_eye_size) / 255.0
            face_img = cv2.resize(face_img, self.target_face_size) / 255.0
            if self.augment:
                left_img = self.datagen.random_transform(left_img)
                right_img = self.datagen.random_transform(right_img)
                face_img = self.datagen.random_transform(face_img)
            left_imgs.append(left_img)
            right_imgs.append(right_img)
            face_imgs.append(face_img)
            labels.append([row['gaze_x'], row['gaze_y']])
        return [np.array(left_imgs), np.array(right_imgs), np.array(face_imgs)], np.array(labels)
    
    def on_epoch_end(self):
        if self.shuffle:
            self.df = self.df.sample(frac=1).reset_index(drop=True)

# =============================================================================
# 2. Build a Multi-Input CNN Model (Inspired by iTracker)
# =============================================================================
def build_multi_input_gaze_model(eye_input_shape=(64, 64, 3), face_input_shape=(224, 224, 3),
                                 learning_rate=1e-4):
    def cnn_branch(input_tensor, name_prefix):
        # A deeper branch: more Conv2D and FC layers, BatchNorm & Dropout.
        x = Conv2D(32, (3, 3), padding='same', name=f"{name_prefix}_conv1")(input_tensor)
        x = BatchNormalization(name=f"{name_prefix}_bn1")(x)
        x = Activation('relu', name=f"{name_prefix}_act1")(x)
        x = MaxPooling2D((2, 2), name=f"{name_prefix}_pool1")(x)

        x = Conv2D(64, (3, 3), padding='same', name=f"{name_prefix}_conv2")(x)
        x = BatchNormalization(name=f"{name_prefix}_bn2")(x)
        x = Activation('relu', name=f"{name_prefix}_act2")(x)
        x = MaxPooling2D((2, 2), name=f"{name_prefix}_pool2")(x)

        x = Conv2D(128, (3, 3), padding='same', name=f"{name_prefix}_conv3")(x)
        x = BatchNormalization(name=f"{name_prefix}_bn3")(x)
        x = Activation('relu', name=f"{name_prefix}_act3")(x)
        x = MaxPooling2D((2, 2), name=f"{name_prefix}_pool3")(x)

        x = Flatten(name=f"{name_prefix}_flatten")(x)
        x = Dense(128, activation='relu', name=f"{name_prefix}_dense1")(x)
        x = Dropout(0.5, name=f"{name_prefix}_dropout1")(x)
        return x

    # Define inputs for left eye, right eye, and face crop.
    left_input = Input(shape=eye_input_shape, name="left_eye")
    right_input = Input(shape=eye_input_shape, name="right_eye")
    face_input = Input(shape=face_input_shape, name="face")
    
    left_features = cnn_branch(left_input, "left")
    right_features = cnn_branch(right_input, "right")
    face_features = cnn_branch(face_input, "face")
    
    # Combine features from both eyes and the full-face crop.
    combined = Concatenate(name="concatenate")([left_features, right_features, face_features])
    x = Dense(256, activation='relu', name="combined_dense1")(combined)
    x = Dropout(0.5, name="combined_dropout1")(x)
    x = Dense(128, activation='relu', name="combined_dense2")(x)
    x = Dropout(0.5, name="combined_dropout2")(x)
    output = Dense(2, activation='linear', name="gaze_output")(x)
    
    model = Model(inputs=[left_input, right_input, face_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])
    return model

# =============================================================================
# 3. Multi-Point Calibration using Polynomial Regression
# =============================================================================
def calibrate_gaze(predictions, true_gaze, degree=2):
    """
    Fits a polynomial regression model to map raw predictions to true gaze coordinates.
    Returns the calibrated predictions, the PolynomialFeatures instance, and the regression model.
    """
    poly = PolynomialFeatures(degree)
    X_poly = poly.fit_transform(predictions)
    reg = LinearRegression()
    reg.fit(X_poly, true_gaze)
    calibrated = reg.predict(X_poly)
    return calibrated, poly, reg

# =============================================================================
# 4. Advanced Face & Eye Cropping for Real-Time Pipeline
# =============================================================================
def extract_face_and_eye_rois(frame, face_mesh_results):
    """
    Uses MediaPipe FaceMesh landmarks to extract:
      - Consistent left and right eye crops.
      - A face crop (face grid) using the bounding box of all face landmarks.
    """
    h, w, _ = frame.shape
    landmarks = face_mesh_results.multi_face_landmarks[0]
    pts = np.array([[int(pt.x * w), int(pt.y * h)] for pt in landmarks.landmark])
    
    # For eyes: using landmarks around the left and right eyes (example indices).
    # In practice, choose indices that consistently bound each eye.
    left_eye_indices = [33, 133, 159, 145]  # example indices for left eye
    right_eye_indices = [362, 263, 386, 374]  # example indices for right eye
    left_eye_pts = pts[left_eye_indices]
    right_eye_pts = pts[right_eye_indices]
    
    # Calculate bounding boxes for eyes with a small margin.
    margin = 5
    lx, ly, lw, lh = cv2.boundingRect(left_eye_pts)
    rx, ry, rw, rh = cv2.boundingRect(right_eye_pts)
    lx = max(lx - margin, 0)
    ly = max(ly - margin, 0)
    rx = max(rx - margin, 0)
    ry = max(ry - margin, 0)
    
    left_eye_roi = frame[ly:ly+lh+2*margin, lx:lx+lw+2*margin]
    right_eye_roi = frame[ry:ry+rh+2*margin, rx:rx+rw+2*margin]
    
    # For face crop: use the bounding rectangle of all face landmarks.
    fx, fy, fw, fh = cv2.boundingRect(pts)
    face_roi = frame[fy:fy+fh, fx:fx+fw]
    
    return left_eye_roi, right_eye_roi, face_roi

# =============================================================================
# 5. Kalman Filter for Temporal Smoothing
# =============================================================================
class GazeKalmanFilter:
    def __init__(self):
        # State: [x, y, vx, vy]
        self.kf = KalmanFilter(dim_x=4, dim_z=2)
        dt = 1.0  # Adjust based on frame rate
        self.kf.F = np.array([[1, 0, dt, 0],
                              [0, 1, 0, dt],
                              [0, 0, 1, 0],
                              [0, 0, 0, 1]])
        self.kf.H = np.array([[1, 0, 0, 0],
                              [0, 1, 0, 0]])
        self.kf.R *= 0.1
        self.kf.P *= 1000.0
        self.kf.Q = np.eye(4) * 0.01
        self.initialized = False

    def update(self, measurement):
        if not self.initialized:
            self.kf.x = np.array([measurement[0], measurement[1], 0, 0]).reshape((4, 1))
            self.initialized = True
        self.kf.predict()
        self.kf.update(np.array(measurement).reshape((2, 1)))
        return self.kf.x[:2].flatten()

# =============================================================================
# 6. Training and Validation Pipeline with Thorough Evaluation
# =============================================================================
def train_and_evaluate(csv_train, csv_val, epochs=50, batch_size=32):
    # Create data generators (ensure training/testing eye/face crops are consistent)
    train_gen = GazeDataGenerator(csv_train, batch_size=batch_size, augment=True)
    val_gen = GazeDataGenerator(csv_val, batch_size=batch_size, augment=False)
    
    # Build the multi-input model
    model = build_multi_input_gaze_model(eye_input_shape=(64, 64, 3), face_input_shape=(224, 224, 3))
    model.summary()
    
    # Use callbacks for hyperparameter tuning (early stopping, checkpointing)
    checkpoint = ModelCheckpoint('best_gaze_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
    
    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs,
                        callbacks=[checkpoint, early_stop])
    
    model.load_weights('best_gaze_model.h5')
    
    # Collect predictions from the validation set for calibration & error evaluation.
    predictions, true_vals = [], []
    for (x, y) in val_gen:
        pred = model.predict(x)
        predictions.append(pred)
        true_vals.append(y)
    predictions = np.vstack(predictions)
    true_vals = np.vstack(true_vals)
    mse = np.mean(np.square(predictions - true_vals))
    mae = np.mean(np.abs(predictions - true_vals))
    print("Validation MSE (raw predictions):", mse)
    print("Validation MAE (raw predictions):", mae)
    
    # Multi-point calibration using polynomial regression (degree 2 as an example)
    calibrated, poly, reg = calibrate_gaze(predictions, true_vals, degree=2)
    mse_calibrated = np.mean(np.square(calibrated - true_vals))
    mae_calibrated = np.mean(np.abs(calibrated - true_vals))
    print("Calibrated Validation MSE:", mse_calibrated)
    print("Calibrated Validation MAE:", mae_calibrated)
    
    # Here, thorough validation should be performed on a held-out set of subjects.
    # You might also compute error in cm or degrees of visual angle.
    return model, poly, reg

# =============================================================================
# 7. Real-Time Gaze Estimation Pipeline with Face-Grid Input
# =============================================================================
def run_real_time_pipeline(model, poly, reg):
    """
    Uses MediaPipe to extract face and eye ROIs consistently.
    Passes the three inputs to the model and applies polynomial calibration and Kalman smoothing.
    """
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)
    
    cap = cv2.VideoCapture(0)
    kalman_filter = GazeKalmanFilter()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)
        if results.multi_face_landmarks:
            # Extract consistent ROIs: left eye, right eye, and face crop.
            left_roi, right_roi, face_roi = extract_face_and_eye_rois(frame, results)
            # Check if crops are valid.
            if left_roi.size == 0 or right_roi.size == 0 or face_roi.size == 0:
                continue
            # Resize crops to the expected input sizes.
            left_roi = cv2.resize(left_roi, (64, 64))
            right_roi = cv2.resize(right_roi, (64, 64))
            face_roi = cv2.resize(face_roi, (224, 224))
            left_roi = cv2.cvtColor(left_roi, cv2.COLOR_BGR2RGB) / 255.0
            right_roi = cv2.cvtColor(right_roi, cv2.COLOR_BGR2RGB) / 255.0
            face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB) / 255.0
            
            left_input = np.expand_dims(left_roi, axis=0)
            right_input = np.expand_dims(right_roi, axis=0)
            face_input = np.expand_dims(face_roi, axis=0)
            # Predict raw gaze coordinates.
            pred = model.predict([left_input, right_input, face_input])[0]
            # Apply polynomial calibration:
            X_poly = poly.transform(pred.reshape(1, -1))
            pred_calibrated = reg.predict(X_poly)[0]
            # Apply temporal smoothing with Kalman Filter.
            pred_smoothed = kalman_filter.update(pred_calibrated)
            
            # Overlay the predicted gaze coordinates on the frame.
            cv2.putText(frame, f"Gaze: ({pred_smoothed[0]:.1f}, {pred_smoothed[1]:.1f})", (30, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.imshow("Real-Time Gaze Estimation", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()

# =============================================================================
# 8. Main: Training, Calibration, and Real-Time Inference
# =============================================================================
if __name__ == "__main__":
    # CSV files should include paths for left eye, right eye, and face crop images,
    # along with gaze_x and gaze_y (gaze labels). For example:
    # left_eye_path,right_eye_path,face_path,gaze_x,gaze_y
    # path/to/left.jpg,path/to/right.jpg,path/to/face.jpg,123,456
    csv_train = "gaze_train.csv"  # Replace with your training CSV
    csv_val = "gaze_val.csv"      # Replace with your validation CSV
    
    # Train the model and obtain calibration parameters.
    model, poly, reg = train_and_evaluate(csv_train, csv_val, epochs=50, batch_size=32)
    
    # Run the real-time pipeline (press 'q' to quit).
    run_real_time_pipeline(model, poly, reg)
