### **Roadmap for Implementing Gaze Estimation Using GazeCapture and Real-Time Inference**

This roadmap outlines how to implement gaze estimation using the GazeCapture dataset, a multi-input CNN model (similar to iTracker), polynomial calibration, and real-time inference with Kalman filtering for smoothing.

---

## **1. Data Preparation & Preprocessing**
### **Step 1.1: Prepare the GazeCapture Dataset**
- Ensure the dataset is structured with:
  - **Left Eye Images** (`left_eye_path`)
  - **Right Eye Images** (`right_eye_path`)
  - **Face Images** (`face_path`)
  - **Gaze Coordinates** (`gaze_x`, `gaze_y`)

- Store this information in a **CSV file** (e.g., `gaze_train.csv`, `gaze_val.csv`).

### **Step 1.2: Implement Data Generator**
- **Use `GazeDataGenerator` to load images in batches.**
- Apply **image augmentation** (random rotation, brightness change).
- Normalize images to **[0,1]** for CNN input.
- Return `(left_eye, right_eye, face) -> (gaze_x, gaze_y)` batches.

---

## **2. Face & Eye Detection**
### **Step 2.1: Face and Eye Detection (Offline & Real-Time)**
- **Use MediaPipe FaceMesh** to detect:
  - **Landmarks for both eyes**
  - **Bounding box for the face**
- Extract consistent **eye crops** and **face crops** using **landmark-based bounding boxes**.

---

## **3. Model Architecture & Training**
### **Step 3.1: Define Multi-Input CNN Model**
- Implement a **CNN branch for each input** (left eye, right eye, face).
- Extract features and **concatenate** them for final gaze estimation.
- Apply **Batch Normalization, Dropout (0.5)** for regularization.
- **Output:** `(x, y)` gaze coordinates.

### **Step 3.2: Train the CNN Model**
- Use **Adam optimizer (`lr=1e-4`)**.
- **Loss Function:** Mean Squared Error (`MSE`).
- **Metrics:** Mean Absolute Error (`MAE`).
- **Callbacks:** 
  - `ModelCheckpoint` (save best weights)
  - `EarlyStopping` (prevent overfitting)

### **Step 3.3: Evaluate Model**
- Compute **raw validation error (MSE, MAE)** before calibration.
- Store model weights (`best_gaze_model.h5`).

---

## **4. Polynomial Calibration**
### **Step 4.1: Apply Multi-Point Calibration**
- Use **Polynomial Regression (`degree=2`)** to map **raw gaze predictions** to **true gaze labels**.
- Fit a **Linear Regression model** on transformed features.
- Apply calibrated transformation to validation predictions.
- Compute **calibrated MSE, MAE**.

---

## **5. Real-Time Gaze Estimation**
### **Step 5.1: Implement Face & Eye Detection in Real-Time**
- Capture live frames using **OpenCV**.
- Detect face & eye landmarks using **MediaPipe FaceMesh**.
- Extract **left eye, right eye, and face** crops **in real-time**.

### **Step 5.2: Run CNN Model for Gaze Estimation**
- Preprocess eye/face crops **(resize, normalize, convert RGB)**.
- **Pass through trained CNN model** to predict `(x, y) gaze` coordinates.
- **Apply polynomial calibration** to adjust predictions.

### **Step 5.3: Apply Kalman Filter for Temporal Smoothing**
- Use a **4D Kalman Filter** to stabilize gaze predictions.
- Reduce noise/jitter from **frame-to-frame gaze shifts**.

### **Step 5.4: Display Results**
- Overlay **smoothed gaze coordinates** on webcam feed.
- Press `'q'` to **exit the real-time inference loop**.

---

## **6. End-to-End Pipeline Execution**
### **Step 6.1: Train the Model (Offline)**
```python
model, poly, reg = train_and_evaluate("gaze_train.csv", "gaze_val.csv")
```
- Train the **multi-input CNN model**.
- Apply **polynomial regression calibration**.
- Store **best model and calibration parameters**.

### **Step 6.2: Run Real-Time Gaze Estimation**
```python
run_real_time_pipeline(model, poly, reg)
```
- Detect **face & eyes**.
- Estimate gaze using the **trained CNN**.
- Apply **calibration & Kalman filter smoothing**.
- Display **real-time gaze tracking**.

---

## **7. Table of Required Libraries & Their Usage**
| **Library**           | **Function in Pipeline** |
|----------------------|----------------------|
| `cv2` (OpenCV) | Image processing, real-time video capture, ROI extraction, preprocessing (resize, normalize). |
| `numpy` | Matrix operations, numerical computations. |
| `pandas` | Handling metadata, reading CSV files (dataset paths, gaze labels). |
| `tensorflow / keras` | Building & training the CNN model for gaze estimation. |
| `mediapipe` | FaceMesh detection for extracting facial landmarks (eyes, face grid). |
| `filterpy.kalman` | Kalman Filter for real-time gaze stabilization (smoothing predictions). |
| `sklearn.linear_model` | Polynomial Regression & Linear Regression for gaze calibration. |
| `sklearn.preprocessing` | `PolynomialFeatures` for gaze calibration (feature transformation). |
| `matplotlib` | Visualization of gaze tracking errors (before & after calibration). |

---

## **Final Summary**
1. **Train a CNN on GazeCapture dataset** (multi-input: left eye, right eye, face).
2. **Validate predictions** and apply **polynomial regression calibration**.
3. **Implement real-time face & eye detection** (MediaPipe) in a webcam feed.
4. **Predict gaze in real-time** using the **trained model**.
5. **Apply polynomial calibration & Kalman filtering** for stability.
6. **Display real-time gaze tracking results** on a live video feed.

---

### ðŸš€ **Next Steps**
Would you like **code improvements** or additional **features (e.g., screen mapping, gaze-based interactions)?** ðŸš€
