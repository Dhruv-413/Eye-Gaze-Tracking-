import cv2
import numpy as np
import mediapipe as mp
import pyautogui
import logging
import sys
import time
import math
import signal
from typing import Tuple, List, Optional
from collections import deque

# ---------------------------------------------------------
# Logging configuration
# ---------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------
# Step 6: Simple Exponential Smoother for Cursor Stabilization
# ---------------------------------------------------------
class SimpleSmoother:
    def __init__(self, alpha: float = 0.7, init_value: float = 0.5):
        self.alpha = alpha
        self.x = init_value
        self.y = init_value

    def update(self, new_x: float, new_y: float) -> Tuple[float, float]:
        self.x = self.alpha * new_x + (1 - self.alpha) * self.x
        self.y = self.alpha * new_y + (1 - self.alpha) * self.y
        return self.x, self.y

# ---------------------------------------------------------
# Main Advanced Eye Tracker Class integrating all steps
# ---------------------------------------------------------
class AdvancedEyeTracker:
    def __init__(self):
        # Basic setup and camera configuration
        pyautogui.FAILSAFE = True  # Keep enabled or adjust as needed.
        self.CAMERA_WIDTH = 640
        self.CAMERA_HEIGHT = 480
        self.MIN_DETECTION_CONFIDENCE = 0.5
        self.MIN_TRACKING_CONFIDENCE = 0.5

        # Calibration and smoothing state
        self.calibration_data: List[Tuple[Tuple[float, float], Tuple[float, float]]] = []
        self.calibrated: bool = False
        self.smoother = SimpleSmoother(alpha=0.7)
        self.gaze_history = deque(maxlen=10)  # For gaze event classification

        # Debug flag to show intermediate windows (set True during development)
        self.debug_mode: bool = False

        # Initialize MediaPipe FaceMesh and camera
        self._setup_mediapipe()
        self._setup_camera()

    # ---------------------------------------------------------
    # Step 1: Face & Eye Detection using MediaPipe FaceMesh
    # ---------------------------------------------------------
    def _setup_mediapipe(self) -> None:
        try:
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=self.MIN_DETECTION_CONFIDENCE,
                min_tracking_confidence=self.MIN_TRACKING_CONFIDENCE
            )
        except Exception as e:
            logger.error(f"Failed to initialize MediaPipe: {e}")
            raise RuntimeError("MediaPipe initialization failed") from e

    def _setup_camera(self) -> None:
        for i in range(3):
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.CAMERA_WIDTH)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.CAMERA_HEIGHT)
                self.cap = cap
                logger.info(f"Camera initialized on index {i}")
                return
        logger.error("No available camera found")
        raise RuntimeError("Failed to initialize camera")

    # ---------------------------------------------------------
    # Step 2: Head Pose Estimation using SolvePnP
    # ---------------------------------------------------------
    def _estimate_head_pose(self, landmarks, frame_shape: Tuple[int, int]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        # Use key landmarks: Nose tip, Chin, Left eye corner, Right eye corner, Left and Right mouth corners.
        image_points = []
        landmark_indices = [1, 152, 33, 263, 61, 291]  # Approximate indices in MediaPipe FaceMesh
        for idx in landmark_indices:
            lm = landmarks.landmark[idx]
            x = lm.x * frame_shape[1]
            y = lm.y * frame_shape[0]
            image_points.append([x, y])
        image_points = np.array(image_points, dtype="double")

        # 3D model points (approximate values in mm)
        model_points = np.array([
            [0.0, 0.0, 0.0],           # Nose tip
            [0.0, -63.6, -12.5],       # Chin
            [-43.3, 32.7, -26.0],      # Left eye left corner
            [43.3, 32.7, -26.0],       # Right eye right corner
            [-28.9, -28.9, -24.1],     # Left mouth corner
            [28.9, -28.9, -24.1]       # Right mouth corner
        ])

        focal_length = frame_shape[1]
        center = (frame_shape[1] / 2, frame_shape[0] / 2)
        camera_matrix = np.array([
            [focal_length, 0, center[0]],
            [0, focal_length, center[1]],
            [0, 0, 1]
        ], dtype="double")
        dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion

        success, rotation_vector, translation_vector = cv2.solvePnP(
            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
        )
        if success:
            return rotation_vector, translation_vector
        else:
            return None, None

    # ---------------------------------------------------------
    # Step 3: Pupil Detection using Adaptive Thresholding + Contour Analysis
    # and Step 7: Adaptive Illumination Handling with CLAHE
    # ---------------------------------------------------------
    def _extract_eye_region(self, landmarks, frame: np.ndarray, eye_indices: List[int]) -> Optional[np.ndarray]:
        try:
            # Get eye landmark coordinates in pixel space
            eye_coords = np.array([
                (int(landmarks.landmark[i].x * frame.shape[1]),
                 int(landmarks.landmark[i].y * frame.shape[0]))
                for i in eye_indices
            ], dtype=np.int32)
            x, y, w, h = cv2.boundingRect(eye_coords)
            if w <= 0 or h <= 0:
                return None
            padding = 10
            x = max(0, x - padding)
            y = max(0, y - padding)
            w = min(frame.shape[1] - x, w + 2 * padding)
            h = min(frame.shape[0] - y, h + 2 * padding)
            eye_roi = frame[y:y+h, x:x+w]
            eye_roi = cv2.resize(eye_roi, (224, 224))
            return eye_roi
        except Exception as e:
            logger.warning(f"Failed to extract eye region: {e}")
            return None

    def _detect_pupil(self, eye_roi: np.ndarray) -> Optional[Tuple[int, int]]:
        try:
            gray = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)
            # Enhance contrast using CLAHE
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)
            # Adaptive thresholding to highlight the dark pupil
            thresh = cv2.adaptiveThreshold(enhanced, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                           cv2.THRESH_BINARY_INV, 11, 5)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours:
                return None
            pupil_contour = max(contours, key=cv2.contourArea)
            M = cv2.moments(pupil_contour)
            if M["m00"] == 0:
                return None
            cX = int(M["m10"] / M["m00"])
            cY = int(M["m01"] / M["m00"])
            if self.debug_mode:
                cv2.circle(eye_roi, (cX, cY), 3, (0, 0, 255), -1)
                cv2.imshow("Pupil Detection", eye_roi)
            return cX, cY
        except Exception as e:
            logger.warning(f"Pupil detection error: {e}")
            return None

    # ---------------------------------------------------------
    # Step 8: Blink Detection using Eye Aspect Ratio (EAR)
    # ---------------------------------------------------------
    def _compute_EAR(self, landmarks, eye_indices: List[int]) -> float:
        def euclidean_dist(pt1, pt2):
            return np.linalg.norm(np.array(pt1) - np.array(pt2))
        pts = [(landmarks.landmark[i].x * self.CAMERA_WIDTH,
                landmarks.landmark[i].y * self.CAMERA_HEIGHT) for i in eye_indices]
        # EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)
        A = euclidean_dist(pts[1], pts[5])
        B = euclidean_dist(pts[2], pts[4])
        C = euclidean_dist(pts[0], pts[3])
        if C == 0:
            return 0.0
        ear = (A + B) / (2.0 * C)
        return ear

    # ---------------------------------------------------------
    # Step 4: Gaze Estimation – Map pupil position (and head pose) to a normalized gaze coordinate.
    # ---------------------------------------------------------
    def _estimate_gaze(self, pupil_center: Tuple[int, int],
                       eye_roi_shape: Tuple[int, int],
                       head_pose: Optional[Tuple[np.ndarray, np.ndarray]]) -> Tuple[float, float]:
        norm_x = pupil_center[0] / eye_roi_shape[1]
        norm_y = pupil_center[1] / eye_roi_shape[0]
        # Optionally adjust using head pose (here, using the norm of rotation vector)
        if head_pose is not None:
            rotation_vector, _ = head_pose
            head_offset = np.linalg.norm(rotation_vector) / 100.0
            norm_x = np.clip(norm_x + head_offset, 0.0, 1.0)
            norm_y = np.clip(norm_y + head_offset, 0.0, 1.0)
        # If calibration data exists, refine mapping using regression
        if self.calibrated:
            norm_x, norm_y = self._apply_calibration((norm_x, norm_y))
        return norm_x, norm_y

    # ---------------------------------------------------------
    # Step 5: Gaze Event Classification using a velocity-based method
    # ---------------------------------------------------------
    def _classify_gaze_event(self, current_gaze: Tuple[float, float]) -> str:
        self.gaze_history.append(current_gaze)
        if len(self.gaze_history) < 2:
            return "Unknown"
        dx = self.gaze_history[-1][0] - self.gaze_history[-2][0]
        dy = self.gaze_history[-1][1] - self.gaze_history[-2][1]
        velocity = math.sqrt(dx*dx + dy*dy)
        if velocity < 0.005:
            return "Fixation"
        else:
            return "Saccade"

    # ---------------------------------------------------------
    # Step 9: Auto-Recalibration using a Linear Regression Mapping
    # ---------------------------------------------------------
    def _apply_calibration(self, raw_gaze: Tuple[float, float]) -> Tuple[float, float]:
        if not self.calibration_data:
            return raw_gaze
        try:
            gaze_points = np.array([p[0] for p in self.calibration_data])
            target_points = np.array([p[1] for p in self.calibration_data])
            gaze_mean = np.mean(gaze_points, axis=0)
            target_mean = np.mean(target_points, axis=0)
            gaze_centered = gaze_points - gaze_mean
            target_centered = target_points - target_mean
            transform = np.dot(np.linalg.pinv(gaze_centered), target_centered)
            gaze_transformed = np.dot(np.array(raw_gaze) - gaze_mean, transform) + target_mean
            return float(gaze_transformed[0]), float(gaze_transformed[1])
        except Exception as e:
            logger.error(f"Calibration transformation failed: {e}")
            return raw_gaze

    # ---------------------------------------------------------
    # Calibration Procedure – Interactive mapping of gaze to screen targets.
    # ---------------------------------------------------------
    def calibrate(self) -> None:
        self.calibration_data.clear()
        screen_width, screen_height = pyautogui.size()
        calibration_points = [
            (0.1, 0.1), (0.5, 0.1), (0.9, 0.1),
            (0.1, 0.5), (0.5, 0.5), (0.9, 0.5),
            (0.1, 0.9), (0.5, 0.9), (0.9, 0.9)
        ]
        logger.info("Calibration started. For each point, look at the target and press 'c'.")
        for point in calibration_points:
            target_x = int(point[0] * screen_width)
            target_y = int(point[1] * screen_height)
            pyautogui.moveTo(target_x, target_y)
            logger.info(f"Focus on target at ({target_x}, {target_y}). Waiting for 'c' key press...")
            while True:
                if cv2.waitKey(1) & 0xFF == ord('c'):
                    break
            ret, frame = self.cap.read()
            if not ret:
                logger.warning("Failed to capture frame during calibration")
                continue
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.face_mesh.process(frame_rgb)
            if not results.multi_face_landmarks:
                continue
            landmarks = results.multi_face_landmarks[0]
            # For calibration, use left eye (landmark indices 33 to 46)
            eye_roi = self._extract_eye_region(landmarks, frame, list(range(33, 46)))
            if eye_roi is None:
                continue
            pupil = self._detect_pupil(eye_roi)
            if pupil is None:
                continue
            raw_gaze = self._estimate_gaze(pupil, eye_roi.shape[:2], None)
            normalized_target = (target_x / screen_width, target_y / screen_height)
            self.calibration_data.append((raw_gaze, normalized_target))
        self.calibrated = len(self.calibration_data) >= 6
        if self.calibrated:
            logger.info(f"Calibration completed with {len(self.calibration_data)} valid points.")
        else:
            logger.warning("Calibration failed. Not enough valid calibration points.")

    # ---------------------------------------------------------
    # Main Loop – Process frames, compute gaze, classify events, and move the cursor.
    # ---------------------------------------------------------
    def run(self) -> None:
        self.running = True
        screen_width, screen_height = pyautogui.size()
        last_move_time = time.time()
        logger.info("Eye tracking started. Press 'q' to quit.")
        try:
            while self.running:
                ret, frame = self.cap.read()
                if not ret:
                    logger.warning("Failed to capture frame")
                    continue

                # Face detection and landmark extraction
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.face_mesh.process(frame_rgb)
                if not results.multi_face_landmarks:
                    continue
                landmarks = results.multi_face_landmarks[0]

                # Head pose estimation
                head_pose = self._estimate_head_pose(landmarks, frame.shape[:2])

                # Blink detection using EAR (using left eye indices as example)
                ear = self._compute_EAR(landmarks, [33, 160, 158, 133, 153, 144])
                if ear < 0.2:
                    logger.info("Blink detected, skipping frame")
                    continue

                # Extract eye region and detect pupil (using left eye)
                eye_roi = self._extract_eye_region(landmarks, frame, list(range(33, 46)))
                if eye_roi is None:
                    continue
                pupil = self._detect_pupil(eye_roi)
                if pupil is None:
                    continue

                # Gaze estimation combining pupil detection and head pose
                raw_gaze = self._estimate_gaze(pupil, eye_roi.shape[:2], head_pose)

                # Classify gaze event (fixation vs. saccade)
                gaze_event = self._classify_gaze_event(raw_gaze)
                if self.debug_mode:
                    cv2.putText(frame, f"Gaze Event: {gaze_event}", (30, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # Stabilize the gaze using the smoother
                smooth_gaze = self.smoother.update(raw_gaze[0], raw_gaze[1])
                screen_x = int(smooth_gaze[0] * screen_width)
                screen_y = int(smooth_gaze[1] * screen_height)

                # Move the cursor at approximately 60Hz
                current_time = time.time()
                if current_time - last_move_time > 0.016:
                    try:
                        pyautogui.moveTo(screen_x, screen_y, duration=0.016)
                    except pyautogui.FailSafeException:
                        logger.warning("Failsafe triggered - cursor near screen edge")
                    last_move_time = current_time

                # Optionally show debug windows
                if self.debug_mode:
                    cv2.imshow("Camera", frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        except Exception as e:
            logger.error(f"Runtime error during eye tracking: {e}")
        finally:
            self.cleanup()

    def cleanup(self) -> None:
        self.running = False
        if hasattr(self, 'cap') and self.cap.isOpened():
            self.cap.release()
        cv2.destroyAllWindows()
        if hasattr(self, 'face_mesh'):
            self.face_mesh.close()
        logger.info("Eye tracker cleaned up successfully")

# ---------------------------------------------------------
# Signal handler for graceful termination
# ---------------------------------------------------------
def signal_handler(sig, frame):
    logger.info("Interrupt signal received. Exiting gracefully...")
    tracker.cleanup()
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)

# ---------------------------------------------------------
# Main entry point
# ---------------------------------------------------------
if __name__ == "__main__":
    try:
        tracker = AdvancedEyeTracker()
        # Enable debug mode if needed for testing:
        # tracker.debug_mode = True
        logger.info("Starting calibration...")
        tracker.calibrate()
        logger.info("Starting eye tracking...")
        tracker.run()
    except Exception as e:
        logger.error(f"Application error: {e}")
        sys.exit(1)
